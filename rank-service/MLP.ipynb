{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MLP Multilayer Perceptron"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "5449d9d2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1182fe8e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb669082",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_item_numeric_features(df):\n",
    "    numeric_feature_names = [\n",
    "        'all_rating_min_max',\n",
    "        'members_min_max',\n",
    "        'aired_from_min_max',\n",
    "        'aired_to_min_max'\n",
    "    ]\n",
    "    \n",
    "    num_df = df[numeric_feature_names]\n",
    "    return num_df.to_numpy()\n",
    "\n",
    "def get_user_numeric_features(df):\n",
    "    numeric_feature_names = [\n",
    "        'user_rating_ave_min_max',\n",
    "        'user_rating_std_min_max',\n",
    "        'user_aired_from_ave_min_max',\n",
    "        'user_aired_to_ave_min_max'\n",
    "    ]\n",
    "    \n",
    "    num_df = df[numeric_feature_names]\n",
    "    return num_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6120aefa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_multihot_feature(df, feat_name):\n",
    "    feat_df = df[[feat_name]]\n",
    "    feat_vecs = feat_df.to_numpy()\n",
    "    feat_vec = np.apply_along_axis(lambda v: v[0], 1, feat_vecs)\n",
    "    return feat_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac4711be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_label(df):\n",
    "    label_df = df[['label']]\n",
    "    return label_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec525757",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_features(df):\n",
    "    return ( \n",
    "        get_multihot_feature(df, 'genres_multihot'),\n",
    "        get_multihot_feature(df, 'user_liked_genres_multihot'),\n",
    "        get_item_numeric_features(df),\n",
    "        get_user_numeric_features(df) \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e8c0cc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Parguet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95d00494",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "905afa31",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_files():\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk('/home/ziyuan/PycharmProjects/gars/dataset/dnn_feat_eng'):\n",
    "        for file in files:\n",
    "            if file.endswith('.parquet'):\n",
    "                filenames.append(os.path.join(root, file))\n",
    "                \n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "724246b6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b47d85ca",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ab2ef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "x1: item categorical feature        \n",
    "x2: user categorical feature          \n",
    "x3: item numeric features          \n",
    "x4: user numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab8d15d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-25 22:29:46.026749: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-25 22:29:46.026805: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74bf2c1b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7854f2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### HParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6ed8456",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# HP_LAYERS = hp.HParam(\"layers\", hp.IntInterval(2, 3))\n",
    "# HP_LAYER_SIZE = hp.HParam(\"layer_size\", hp.Discrete([64, 128, 256]))\n",
    "# HP_LEARN_RATE = hp.HParam(\"learn_rate\", hp.Discrete([0.001, 0.003, 0.01]))\n",
    "\n",
    "HP_LAYERS = hp.HParam(\"layers\", hp.IntInterval(2, 3))\n",
    "HP_LAYER_SIZE = hp.HParam(\"layer_size\", hp.Discrete([64, 128, 256]))\n",
    "HP_LEARN_RATE = hp.HParam(\"learn_rate\", hp.Discrete([0.001, 0.003, 0.01]))\n",
    "\n",
    "HPARAMS = [\n",
    "    HP_LAYERS,\n",
    "    HP_LAYER_SIZE,\n",
    "    HP_LEARN_RATE\n",
    "]\n",
    "\n",
    "METRICS = [\n",
    "    hp.Metric(\n",
    "        \"batch_loss\",\n",
    "        group=\"train\",\n",
    "        display_name=\"loss (train)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"loss\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"loss (val)\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a101f73f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(x1_shape, x2_shape, x3_shape, x4_shape, hparams):\n",
    "    x1_input = keras.layers.Input(shape=(x1_shape,))\n",
    "    x2_input = keras.layers.Input(shape=(x2_shape,))\n",
    "    x3_input = keras.layers.Input(shape=(x3_shape,))\n",
    "    x4_input = keras.layers.Input(shape=(x4_shape,))\n",
    "    \n",
    "    # compact embedding for x1 and x2\n",
    "    compact_x1 = keras.layers.Dense(10)(x1_input)\n",
    "    compact_x2 = keras.layers.Dense(10)(x2_input)\n",
    "    \n",
    "    # concat all\n",
    "    merge = keras.layers.concatenate([compact_x1, compact_x2, x3_input, x4_input])\n",
    "    \n",
    "    # hidden layers\n",
    "    h_input = merge\n",
    "    for _ in range(hparams[HP_LAYERS]):\n",
    "        h = keras.layers.Dense(hparams[HP_LAYER_SIZE], activation='relu')(h_input)\n",
    "        h_input = h\n",
    "    \n",
    "    # output\n",
    "    output = keras.layers.Dense(1, activation='sigmoid')(h_input)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[x1_input, x2_input, x3_input, x4_input], outputs=output)\n",
    "    \n",
    "    # optimizer\n",
    "    opt = keras.optimizers.Adam(learning_rate=hparams[HP_LEARN_RATE])\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b4d4c2a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfd51dae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78b002c6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Data and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa1d8200",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_x1s = []\n",
    "test_x2s = []\n",
    "test_x3s = []\n",
    "test_x4s = []\n",
    "test_ys = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3602f354",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_files():\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk('/home/ziyuan/PycharmProjects/gars/dataset/dnn_feat_eng'):\n",
    "        for file in files:\n",
    "            if file.endswith('.parquet'):\n",
    "                filenames.append(os.path.join(root, file))\n",
    "                \n",
    "    return filenames\n",
    "\n",
    "filenames = data_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bed625c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_model(model_id, hparams):\n",
    "    # build model\n",
    "    model = build_model(43, 43, 4, 4, hparams)\n",
    "    print(f\"model id: {model_id}:\")\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "\n",
    "    # config hparam logs\n",
    "    log_filename = f\"{model_id}\"\n",
    "    for h in hparams:\n",
    "        log_filename += f\"_{h.name}-{hparams[h]}\"\n",
    "    \n",
    "    log_dir = os.path.join(\"hparams\", log_filename)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir = log_dir,\n",
    "        update_freq = 10,\n",
    "        profile_batch = 0\n",
    "    )\n",
    "    hparams_callback = hp.KerasCallback(log_dir, hparams)\n",
    "    \n",
    "    # train model\n",
    "    for filename in filenames[:1]:\n",
    "        df = pd.read_parquet(filename)\n",
    "\n",
    "        # shuffle and split train and test\n",
    "        train_df = df\n",
    "#         train_df = df.sample(frac=0.8, random_state=666)\n",
    "#         test_df = df.drop(train_df.index)\n",
    "\n",
    "        # get features\n",
    "        train_x1, train_x2, train_x3, train_x4 = get_all_features(train_df)\n",
    "#         val_x1, val_x2, val_x3, val_x4 = get_all_features(test_df)\n",
    "\n",
    "        # get label\n",
    "        train_y = get_label(train_df)\n",
    "#         val_y = get_label(test_df)\n",
    "\n",
    "        print('training on new dataset')\n",
    "\n",
    "        model.fit(\n",
    "            [train_x1, train_x2, train_x3, train_x4], \n",
    "            train_y, \n",
    "            validation_split=0.2,\n",
    "            batch_size=16, \n",
    "            epochs=4,\n",
    "            callbacks=[tensorboard_callback, hparams_callback]\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60cc3dd2",
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_params():\n",
    "    with tf.summary.create_file_writer('hparams').as_default():\n",
    "            hp.hparams_config(hparams=HPARAMS, metrics=METRICS)\n",
    "            \n",
    "    model_id = 0\n",
    "    for layers in range(HP_LAYERS.domain.min_value, HP_LAYERS.domain.max_value + 1):\n",
    "        for size in HP_LAYER_SIZE.domain.values:\n",
    "            for rate in HP_LEARN_RATE.domain.values:\n",
    "                hparams = {\n",
    "                    HP_LAYERS: layers,\n",
    "                    HP_LAYER_SIZE: size,\n",
    "                    HP_LEARN_RATE: rate\n",
    "                }\n",
    "\n",
    "                run_model(model_id, hparams)\n",
    "                model_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbcd072b",
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model id: 0:\n",
      "{'layers': 2, 'layer_size': 64, 'learn_rate': 0.001}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5763 - accuracy: 0.6977 - val_loss: 0.5758 - val_accuracy: 0.7031\n",
      "Epoch 2/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5364 - accuracy: 0.7297 - val_loss: 0.5865 - val_accuracy: 0.7028\n",
      "Epoch 3/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5215 - accuracy: 0.7422 - val_loss: 0.5793 - val_accuracy: 0.6988\n",
      "Epoch 4/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5130 - accuracy: 0.7470 - val_loss: 0.5799 - val_accuracy: 0.7068\n",
      "model id: 1:\n",
      "{'layers': 2, 'layer_size': 64, 'learn_rate': 0.003}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5765 - accuracy: 0.6961 - val_loss: 0.5808 - val_accuracy: 0.6987\n",
      "Epoch 2/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5368 - accuracy: 0.7297 - val_loss: 0.5729 - val_accuracy: 0.7125\n",
      "Epoch 3/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5257 - accuracy: 0.7377 - val_loss: 0.5637 - val_accuracy: 0.7172\n",
      "Epoch 4/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5187 - accuracy: 0.7432 - val_loss: 0.5659 - val_accuracy: 0.7221\n",
      "model id: 2:\n",
      "{'layers': 2, 'layer_size': 64, 'learn_rate': 0.01}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5893 - accuracy: 0.6866 - val_loss: 0.5917 - val_accuracy: 0.6943\n",
      "Epoch 2/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5534 - accuracy: 0.7197 - val_loss: 0.5759 - val_accuracy: 0.7041\n",
      "Epoch 3/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5426 - accuracy: 0.7271 - val_loss: 0.5749 - val_accuracy: 0.7138\n",
      "Epoch 4/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5354 - accuracy: 0.7327 - val_loss: 0.5738 - val_accuracy: 0.7028\n",
      "model id: 3:\n",
      "{'layers': 2, 'layer_size': 128, 'learn_rate': 0.001}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1759/1759 [==============================] - 3s 1ms/step - loss: 0.5750 - accuracy: 0.6972 - val_loss: 0.5788 - val_accuracy: 0.7007\n",
      "Epoch 2/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5320 - accuracy: 0.7317 - val_loss: 0.5634 - val_accuracy: 0.7148\n",
      "Epoch 3/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5182 - accuracy: 0.7430 - val_loss: 0.5693 - val_accuracy: 0.7071\n",
      "Epoch 4/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5094 - accuracy: 0.7492 - val_loss: 0.5653 - val_accuracy: 0.7190\n",
      "model id: 4:\n",
      "{'layers': 2, 'layer_size': 128, 'learn_rate': 0.003}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1759/1759 [==============================] - 3s 1ms/step - loss: 0.5737 - accuracy: 0.7034 - val_loss: 0.6219 - val_accuracy: 0.6585\n",
      "Epoch 2/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5398 - accuracy: 0.7287 - val_loss: 0.5688 - val_accuracy: 0.7119\n",
      "Epoch 3/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5266 - accuracy: 0.7396 - val_loss: 0.5835 - val_accuracy: 0.7017\n",
      "Epoch 4/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5210 - accuracy: 0.7447 - val_loss: 0.5656 - val_accuracy: 0.7162\n",
      "model id: 5:\n",
      "{'layers': 2, 'layer_size': 128, 'learn_rate': 0.01}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1759/1759 [==============================] - 3s 1ms/step - loss: 0.5913 - accuracy: 0.6860 - val_loss: 0.5898 - val_accuracy: 0.6853\n",
      "Epoch 2/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5551 - accuracy: 0.7151 - val_loss: 0.6115 - val_accuracy: 0.6791\n",
      "Epoch 3/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5389 - accuracy: 0.7294 - val_loss: 0.5868 - val_accuracy: 0.7018\n",
      "Epoch 4/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5332 - accuracy: 0.7347 - val_loss: 0.5669 - val_accuracy: 0.7156\n",
      "model id: 6:\n",
      "{'layers': 2, 'layer_size': 256, 'learn_rate': 0.001}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1759/1759 [==============================] - 3s 2ms/step - loss: 0.5722 - accuracy: 0.7026 - val_loss: 0.5802 - val_accuracy: 0.6971\n",
      "Epoch 2/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5351 - accuracy: 0.7311 - val_loss: 0.5663 - val_accuracy: 0.7088\n",
      "Epoch 3/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5201 - accuracy: 0.7444 - val_loss: 0.5625 - val_accuracy: 0.7106\n",
      "Epoch 4/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5122 - accuracy: 0.7489 - val_loss: 0.5687 - val_accuracy: 0.7193\n",
      "model id: 7:\n",
      "{'layers': 2, 'layer_size': 256, 'learn_rate': 0.003}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1759/1759 [==============================] - 3s 2ms/step - loss: 0.5798 - accuracy: 0.6984 - val_loss: 0.5739 - val_accuracy: 0.7020\n",
      "Epoch 2/4\n",
      "1759/1759 [==============================] - 3s 1ms/step - loss: 0.5405 - accuracy: 0.7274 - val_loss: 0.6047 - val_accuracy: 0.7001\n",
      "Epoch 3/4\n",
      "1759/1759 [==============================] - 3s 1ms/step - loss: 0.5268 - accuracy: 0.7393 - val_loss: 0.5634 - val_accuracy: 0.7167\n",
      "Epoch 4/4\n",
      "1759/1759 [==============================] - 3s 1ms/step - loss: 0.5189 - accuracy: 0.7431 - val_loss: 0.5602 - val_accuracy: 0.7219\n",
      "model id: 8:\n",
      "{'layers': 2, 'layer_size': 256, 'learn_rate': 0.01}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1759/1759 [==============================] - 3s 1ms/step - loss: 0.5957 - accuracy: 0.6810 - val_loss: 0.6168 - val_accuracy: 0.6514\n",
      "Epoch 2/4\n",
      "1759/1759 [==============================] - 3s 1ms/step - loss: 0.5583 - accuracy: 0.7146 - val_loss: 0.5866 - val_accuracy: 0.6899\n",
      "Epoch 3/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5444 - accuracy: 0.7257 - val_loss: 0.5658 - val_accuracy: 0.7112\n",
      "Epoch 4/4\n",
      "1759/1759 [==============================] - 3s 1ms/step - loss: 0.5347 - accuracy: 0.7329 - val_loss: 0.5767 - val_accuracy: 0.7004\n",
      "model id: 9:\n",
      "{'layers': 3, 'layer_size': 64, 'learn_rate': 0.001}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1759/1759 [==============================] - 3s 1ms/step - loss: 0.5829 - accuracy: 0.6929 - val_loss: 0.5776 - val_accuracy: 0.6995\n",
      "Epoch 2/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5371 - accuracy: 0.7290 - val_loss: 0.5789 - val_accuracy: 0.7032\n",
      "Epoch 3/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5197 - accuracy: 0.7430 - val_loss: 0.5669 - val_accuracy: 0.7041\n",
      "Epoch 4/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5100 - accuracy: 0.7522 - val_loss: 0.5668 - val_accuracy: 0.7206\n",
      "model id: 10:\n",
      "{'layers': 3, 'layer_size': 64, 'learn_rate': 0.003}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1759/1759 [==============================] - 3s 1ms/step - loss: 0.5795 - accuracy: 0.6955 - val_loss: 0.5856 - val_accuracy: 0.6922\n",
      "Epoch 2/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5409 - accuracy: 0.7289 - val_loss: 0.5718 - val_accuracy: 0.7072\n",
      "Epoch 3/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5289 - accuracy: 0.7352 - val_loss: 0.5642 - val_accuracy: 0.7123\n",
      "Epoch 4/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5212 - accuracy: 0.7420 - val_loss: 0.5857 - val_accuracy: 0.7048\n",
      "model id: 11:\n",
      "{'layers': 3, 'layer_size': 64, 'learn_rate': 0.01}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1759/1759 [==============================] - 3s 1ms/step - loss: 0.5930 - accuracy: 0.6863 - val_loss: 0.5946 - val_accuracy: 0.6923\n",
      "Epoch 2/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5600 - accuracy: 0.7138 - val_loss: 0.6127 - val_accuracy: 0.6839\n",
      "Epoch 3/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5459 - accuracy: 0.7274 - val_loss: 0.5651 - val_accuracy: 0.7125\n",
      "Epoch 4/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5358 - accuracy: 0.7330 - val_loss: 0.5579 - val_accuracy: 0.7260\n",
      "model id: 12:\n",
      "{'layers': 3, 'layer_size': 128, 'learn_rate': 0.001}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1759/1759 [==============================] - 3s 1ms/step - loss: 0.5765 - accuracy: 0.6972 - val_loss: 0.5745 - val_accuracy: 0.7048\n",
      "Epoch 2/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5320 - accuracy: 0.7335 - val_loss: 0.5720 - val_accuracy: 0.7113\n",
      "Epoch 3/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5204 - accuracy: 0.7426 - val_loss: 0.5714 - val_accuracy: 0.7098\n",
      "Epoch 4/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5114 - accuracy: 0.7480 - val_loss: 0.5716 - val_accuracy: 0.7102\n",
      "model id: 13:\n",
      "{'layers': 3, 'layer_size': 128, 'learn_rate': 0.003}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1759/1759 [==============================] - 3s 1ms/step - loss: 0.5790 - accuracy: 0.6981 - val_loss: 0.5960 - val_accuracy: 0.6849\n",
      "Epoch 2/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5408 - accuracy: 0.7290 - val_loss: 0.5687 - val_accuracy: 0.6998\n",
      "Epoch 3/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5278 - accuracy: 0.7399 - val_loss: 0.5730 - val_accuracy: 0.7132\n",
      "Epoch 4/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5230 - accuracy: 0.7433 - val_loss: 0.5561 - val_accuracy: 0.7186\n",
      "model id: 14:\n",
      "{'layers': 3, 'layer_size': 128, 'learn_rate': 0.01}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1759/1759 [==============================] - 3s 1ms/step - loss: 0.5966 - accuracy: 0.6850 - val_loss: 0.6082 - val_accuracy: 0.6833\n",
      "Epoch 2/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5601 - accuracy: 0.7151 - val_loss: 0.5975 - val_accuracy: 0.7003\n",
      "Epoch 3/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5464 - accuracy: 0.7248 - val_loss: 0.5794 - val_accuracy: 0.6895\n",
      "Epoch 4/4\n",
      "1759/1759 [==============================] - 2s 1ms/step - loss: 0.5446 - accuracy: 0.7270 - val_loss: 0.5845 - val_accuracy: 0.6993\n",
      "model id: 15:\n",
      "{'layers': 3, 'layer_size': 256, 'learn_rate': 0.001}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1759/1759 [==============================] - 3s 2ms/step - loss: 0.5761 - accuracy: 0.7012 - val_loss: 0.5822 - val_accuracy: 0.6939\n",
      "Epoch 2/4\n",
      "1759/1759 [==============================] - 3s 2ms/step - loss: 0.5359 - accuracy: 0.7305 - val_loss: 0.5842 - val_accuracy: 0.6914\n",
      "Epoch 3/4\n",
      "1759/1759 [==============================] - 3s 2ms/step - loss: 0.5207 - accuracy: 0.7446 - val_loss: 0.5695 - val_accuracy: 0.7123\n",
      "Epoch 4/4\n",
      "1759/1759 [==============================] - 3s 2ms/step - loss: 0.5126 - accuracy: 0.7494 - val_loss: 0.5712 - val_accuracy: 0.7078\n",
      "model id: 16:\n",
      "{'layers': 3, 'layer_size': 256, 'learn_rate': 0.003}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1759/1759 [==============================] - 3s 2ms/step - loss: 0.5820 - accuracy: 0.6941 - val_loss: 0.5796 - val_accuracy: 0.6977\n",
      "Epoch 2/4\n",
      "1759/1759 [==============================] - 3s 2ms/step - loss: 0.5432 - accuracy: 0.7286 - val_loss: 0.5648 - val_accuracy: 0.7160\n",
      "Epoch 3/4\n",
      "1759/1759 [==============================] - 3s 2ms/step - loss: 0.5280 - accuracy: 0.7356 - val_loss: 0.5661 - val_accuracy: 0.7138\n",
      "Epoch 4/4\n",
      "1759/1759 [==============================] - 3s 2ms/step - loss: 0.5248 - accuracy: 0.7402 - val_loss: 0.5632 - val_accuracy: 0.7138\n",
      "model id: 17:\n",
      "{'layers': 3, 'layer_size': 256, 'learn_rate': 0.01}\n",
      "training on new dataset\n",
      "Epoch 1/4\n",
      "1759/1759 [==============================] - 3s 2ms/step - loss: 0.6047 - accuracy: 0.6760 - val_loss: 0.5886 - val_accuracy: 0.6919\n",
      "Epoch 2/4\n",
      "1759/1759 [==============================] - 3s 2ms/step - loss: 0.5627 - accuracy: 0.7139 - val_loss: 0.5882 - val_accuracy: 0.6922\n",
      "Epoch 3/4\n",
      "1759/1759 [==============================] - 3s 2ms/step - loss: 0.5489 - accuracy: 0.7235 - val_loss: 0.5750 - val_accuracy: 0.7017\n",
      "Epoch 4/4\n",
      "1759/1759 [==============================] - 3s 2ms/step - loss: 0.5406 - accuracy: 0.7335 - val_loss: 0.5960 - val_accuracy: 0.7035\n"
     ]
    }
   ],
   "source": [
    "test_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e581eddd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-898d4bb4e84b6d25\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-898d4bb4e84b6d25\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48e00572",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ac56a05",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e99e3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test_x1 = np.vstack(test_x1s)\n",
    "# test_x2 = np.vstack(test_x2s)\n",
    "# test_x3 = np.vstack(test_x3s)\n",
    "# test_x4 = np.vstack(test_x4s)\n",
    "# test_y = np.vstack(test_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a8a150",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test_loss, test_accuracy = model.evaluate([test_x1, test_x2, test_x3, test_x4], test_y)\n",
    "#\n",
    "# print('\\n\\nTest Loss {}, Test Accuracy {}'.format(test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ddae2e",
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# n = 1\n",
    "# g = 0\n",
    "#\n",
    "# for i in range(n):\n",
    "#     expect = test_y[i][0]\n",
    "#     x1 = test_x1[i]\n",
    "#     x2 = test_x2[i]\n",
    "#     x3 = test_x3[i]\n",
    "#     x4 = test_x4[i]\n",
    "#     xs = [ np.array([x1]), np.array([x2]), np.array([x3]), np.array([x4]) ]\n",
    "#     print('xs')\n",
    "#     print(xs)\n",
    "#\n",
    "#     predict = model.predict(xs)\n",
    "#     predict = predict[0][0]\n",
    "#     if predict > 0.5:\n",
    "#         predict = 1\n",
    "#     else:\n",
    "#         predict = 0\n",
    "#\n",
    "#     if predict == expect:\n",
    "#         g += 1.0\n",
    "#\n",
    "# #     print(f'Expect {expect}, predict {predict}')\n",
    "#\n",
    "#\n",
    "# print()\n",
    "# print('accuracy:')\n",
    "# print(g / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2afd28",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6440a4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model.save('mlp_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d15fff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}